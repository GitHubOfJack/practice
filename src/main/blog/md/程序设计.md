https://www.cnblogs.com/crazymakercircle/p/14367425.html

https://blog.csdn.net/qwe86314/article/details/94552801  eureka工作原理

Eurka 工作流程
了解完 Eureka 核心概念，自我保护机制，以及集群内的工作原理后，我们来整体梳理一下 Eureka 的工作流程：

1、Eureka Server 启动成功，等待服务端注册。在启动过程中如果配置了集群，集群之间定时通过 Replicate 同步注册表，每个 Eureka Server 都存在独立完整的服务注册表信息

2、Eureka Client 启动时根据配置的 Eureka Server 地址去注册中心注册服务

3、Eureka Client 会每 30s 向 Eureka Server 发送一次心跳请求，证明客户端服务正常

4、当 Eureka Server 90s 内没有收到 Eureka Client 的心跳，注册中心则认为该节点失效，会注销该实例

5、单位时间内 Eureka Server 统计到有大量的 Eureka Client 没有上送心跳，则认为可能为网络异常，进入自我保护机制，不再剔除没有上送心跳的客户端

6、当 Eureka Client 心跳请求恢复正常之后，Eureka Server 自动退出自我保护模式

7、Eureka Client 定时全量或者增量从注册中心获取服务注册表，并且将获取到的信息缓存到本地

8、服务调用时，Eureka Client 会先从本地缓存找寻调取的服务。如果获取不到，先从注册中心刷新注册表，再同步到本地缓存

9、Eureka Client 获取到目标服务器信息，发起服务调用

10、Eureka Client 程序关闭时向 Eureka Server 发送取消请求，Eureka Server 将实例从注册表中删除

这就是Eurka基本工作流程



feign工作原理

​	feign封装了ribbon和hystrix

​	@feignClient(name="指定服务在eureka上的名字" path="访问路径" fellbackFactory="熔断方案")--这些都是接口

​	@enableFeignClient（指定加载那些feignClient）--会把制定的feignclient定义的类加入到ioc容器里面

​	通过jdk动态代理机制生成代理对象，通过代理对象获取请求参数，同时可以获取eureka上的服务器的ip信息，通过负载均衡算法，再加上http工具（okhttp\httpclient\httUrlConnection）生成最终的请求对象，进行访问。

ribbon利用了RestTemplate的拦截器机制，在拦截器中实现ribbon的负载均衡。负载均衡的基本实现就是利用applicationName从服务注册中心获取可用的服务地址列表，然后通过一定算法负载，决定使用哪一个服务地址来进行http调用



![hystrix原理](..\image\hystrix原理.png)

hystrix限流可以采用线程池限流和信号量限流

|        | 线程切换 | 支持异步 | 支持超时 | 支持熔断 | 限流 | 开销 |
| ------ | -------- | -------- | -------- | -------- | ---- | ---- |
| 信号量 | 否       | 否       | 否       | 是       | 是   | 小   |
| 线程池 | 是       | 是       | 是       | 是       | 是   | 大   |





本地缓存Map<String, Object>

Guava-cache 线程安全（底层是CurrentMap,分段锁保证线程安全），设置并发数量，设置默认大小，最大大小，过期策略（写入多长时间之后失效、读写多长时间之后失效、写入多长时间之后刷新等），不是通过增加线程删除，是访问的时候删除。同时，写入更新的时候只有一个线程去加载数据，其余的线程返回旧值（这可能会出现数据不一致的问题）。

Caffeine-cache

Ehcache

- - 跟expire的区别是，指定时间过后，expire是remove该key，下次访问是同步去获取返回新值；而refresh则是指定时间后，不会remove该key，下次访问会触发刷新，新值没有回来时返回旧值







redis版本都是 3.X

mysql版本是5.6和5.7





总合同数量：1090万

正常合同数量：106万

逾期合同数量：152万

正常结清合同数量：448万

提前结清合同数量：219万

减免结清合同数量：22万



件均：3万7



还款计划总数：1亿7689万

正常合同还款计划总数：2147万

逾期合同还款计划总数：3872万

正常结清还款计划总数：5381万

提前结清还款计划总数：626万

减免结清还款计划总数：5318万





每日还款数：大概日活22万+（日还款），2月28日，日活40万+



最高放款量：12000多笔，都是小短放款

| 221845 | 2021-03-19 |
| ------ | ---------- |
| 223546 | 2021-03-18 |
| 229074 | 2021-03-17 |
| 236727 | 2021-03-16 |
| 230719 | 2021-03-15 |
| 193081 | 2021-03-14 |
| 199975 | 2021-03-13 |
| 236290 | 2021-03-12 |
| 225118 | 2021-03-11 |
| 248155 | 2021-03-10 |
| 230232 | 2021-03-09 |

| 225668 | 2021-03-08 |
| ------ | ---------- |
| 202026 | 2021-03-07 |
| 193851 | 2021-03-06 |
| 230183 | 2021-03-05 |
| 245270 | 2021-03-04 |
| 220203 | 2021-03-03 |
| 233718 | 2021-03-02 |
| 268946 | 2021-03-01 |
| 377665 | 2021-02-28 |
| 239141 | 2021-02-27 |
| 250695 | 2021-02-26 |
| 262829 | 2021-02-25 |

总用户数量：宜人贷注册用户1个亿，贷款用户1000万，合同数量1760115



如果按照260W合同  40分钟内完成计算

260W/(40*60) = 1083个合同/s

如果是4台服务器，一台的tps=270左右，这个是io密集型的任务，如果有4台服务器，服务器配置是4c8g，每台服务器的线程数配置的是40，每个线程的tps是7，每个合同的处理时间是142毫秒（这个时间压力很大）

如果是6台服务器，每台线程还是40，每个线程的tps是4.5，每个合同的处理时间是222毫秒

如果是8台服务器，每台服务器线程是40，每个线程的tps是3.4，每个合同的处理时间是290毫秒



调用数据库的时间100毫秒，调用产品中心的时间100毫秒，至少两次数据库查询，1次产品中心查询，所以8台比较合理







明天要做的事情：

1 分库分表（唯一主键的生成方案、分表的联合查询、排序查询、事务等操作）

ID生成方案：

​	1 请求同一个表，返回主键，根据主键取模，再插入到对应的数据库

​	2 不通的表设置相同的步长，步长=表的个数

​	3 请求同一个表，返回主键后，可以使用1000*n+1到1000*（n+1）这种范围的形式（这三种形式都受限于数据库的性能）

​	4 可以使用redis的incr/incrby自增原子命令（性能比数据库性能高，缺点是：redis宕机之后找出最新的id比较麻烦）

​	5 uuid（性能高，本地生成，没有网络消耗，缺点：uuid太长，占用空间大，无序会导致mysql插入性能变差）

​	6 snowflake（雪花算法）

​		long类型是8个字节64位，符号为是0，表示整数，紧接着41位是毫秒数（从算法开始使用到截止可以使用2^41-1的毫秒数时间共69年），再接着10位是，集群id+机器id，最多支持2^10=1024台服务器，最后12位是序号，每毫秒最多生产2^12=4096个序列号，即每秒支持409万的并发数（优点：整个数字是递增的，数据库插入效率高，不依赖于三方系统，本地生成，没有网络消耗，依赖时间钟，如果机器时间钟回拨，可能会出现发号重复的问题）如果各个服务器时间不一致，也会出现页分裂的问题

​	7 我们业务中常用的(订单号、交易单号的生成规则)：yyyyMMddHHMIssSSS+4位随机数（支持千万并发，确定：长度太长、可能重复，不适合作为主键使用）





2 系统的qps（以日终为例）

3 还款的系统交互图（以及可能的问题）





mysql优化的6种方案：

1 加缓存减少读压力

2 wal预写日志（是否可以把数据放入redis同步记录日志，最后同步到数据库，减轻写的压力）

3 重新设计表结构（比如增加冗余字段，减少io次数，减少关联查询等操作）

4 打开慢日志配置，输出慢日志sql，分析慢sql

5 打开sql分析配置，开始sql性能分析

6 分表

7 分库





页分裂：更新和插入操作，会在索引上加悲观锁，影响性能

页合并：更新和删除操作，会在索引上加悲观锁，影响性能







每日需要更新的表：合同表（合同状态）、还款计划表（应还罚滞纳、应还总金额、逾期状态、逾期天数）

扫描的条件：1 还款计划为主表

每日更新数据量：合同表是正常变逾期的合同，还款计划是：还款截止日是昨天的和逾期未结清的还款计划





日终的方案：

1 多台服务器分片执行？  2 主节点分派任务？

2 每种方案的数据扫描机制？

3 每种方案的优缺点，怎么解决的？

4 oracle->mysql架构升级？



目前的方案：

数据库oracle，服务器4台，4c8g，数据库表，合同表和还款计划表均是单表，生成合同时，有一个dealNum字段（生成规则是合同号取模，合同号是自增的）



shardingjdbc分片

1 标准分片：单列分片  >= <= = in between等，必须实现精准分片，范围分片可以不实现，但是在between查询会查询全部表，需要自己定义分片算法

2 多列分片：多列分片 >= <= = in between等，需要自己定义算法

3 groovy表达式分片，仅支持单列分片，仅对= in有效

4 hint分片，没用过，大体是使用了表中没有的列进行分表，threadlocal



16c32g

96c oracle





1 sync升级过程

2 spring解决循环依赖

3 dcl单例问题





1 定时任务升级过程

​	原方案：4台服务器4c8g，定时任务启动时选主，主节点启动一个线程池40个线程，主节点负责从数据库读取数据，并分发给线程池中的线程，线程通过http接口的调用，执行定时任务。（主节点是单节点的问题，所有压力都在主节点，比较耗时）

​	改进方案一：4台服务器，每台服务器都启动一个线程池，主节点调用http请求，子节点接收到请求之后，放入线程池异步处理。（主节点是单节点问题，压力相对平均，通过ng做负载均衡策略）

 	改进方案二：定时任务启动时选主，主节点把要处理的数据放入到mq中，主节点和子节点都拉取相应的消息进行处理。（主节点是单节问题，压力相对平均，可以承受更大的数据，通过rocketmq做负载均衡策略，引入了三方的服务，提高了复杂性）

​	改进方案三：数据分片，选主，主节点分配任务，子节点监听时间，负责处理，同时有zk锁，redis锁，

​	改进方案四：分表、spring-batch

















Guava-bloomFilter

Redis-redission可以实现分布式锁、也有boolmFilter

限流的方案：1 技术限流（本地限流guava-ratelimiter、分布式限流redis-cell、nginx限流） 2 业务限流（抢购时间从1个变成多个、抢购需要进行验证码确认） 3 使用mq限流



大流量、高并发如果不采用流量管控，会导致连接资源耗尽、分布式缓存撑爆、数据库吞吐量下降等问题，最终导致服务雪崩。

应对大流量、高并发的手段有：扩容、动静分离、缓存（读写分离）、服务限流、服务降级、熔断等措施。







积分商城秒杀项目

宜信用户、连续7天登陆就可以参加iphone手机秒杀（使用bitmap）setbit getbit bitcount bitop and|or  1亿多数据  15m   

用户每次登陆：setbit key=秒杀活动:20210101 userid 1（userid必须是对应的整数）

bitcount 秒杀活动:20210101                         返回一个参数活动的用户的大概数量（根据这个值进行相应的评估和压力测试）

7天之后：bitop and and-result 秒杀活动:20210101 秒杀活动:20210102 ... 秒杀活动:20210107

jemeter压力测试：单机qps是1800左右  线上4台服务器

两个限流地方(1 ng配置限流 2 redis缓存限流)

服务启动会初始化一个key=productid value=10

二级缓存，本地缓存用来存储库存是否完毕，如果完毕，如果完毕直接返回。redis缓存用来存储库存，每个请求直接decr，如果decr的结果<0，则incr，incr之后如果>0，则需要修改本地缓存。（还必须有一个线程定时从redis缓存中更新本地缓存，本地缓存不配置失效策略）。如果还有库存，则直接调用mysql新增订单和库存。如果mysql更新失败（sql update t set stock-1 where pid = 1 and stock > 0;），则执行redis中的incr，同时判定是否需要修改本地缓存。 秒杀完毕之后，会发送支付通知，通知用户支付。



站在架构角度、在海量数据的并发再看秒杀

大促或者秒杀系统设计（扩容、动静分离、缓存、限流、降级、熔断）

大数据量高并发情况下保证数据一致性的问题（超卖问题）

n多的产品信息、sku信息、库存信息、订单信息、支付信息等

漏斗原理：前端到后端的用户数量一直减少，最后保证是串行执行

站在架构角度看

1 浏览器有缓存

2 使用cdn，动静分离-减少服务器的宽带压力

3 使用nginx-做负载均衡和动静分离

4 读的问题：使用redis做缓存（但是不管是使用哨兵模式、集群模式、一致性hash实现的集群模式，对于热点的key都存在单点压力问题，所以采用二级缓存，本地缓存+分布式缓存）（本地缓存和分布式缓存不一致的问题，1 可以通过mq解决，2 可以通过定时任务解决 3 通过zk解决，实效性问题。目前通过定时任务解决，不需要再引入三方服务，如果出现了不一致的问题，通过写流程校验）。

5 写的问题：数据库是通过innodb的行锁解决（并且要加入stock>0的条件），数据库压力大，可以放到缓存中。通过缓存的写，降低一部分压力，也可以通过mq等异步措施，异步的处理后续订单，如果处理失败，更新缓存继续抢购。

接口要有开关，可以做到动态关闭和开启。

一定要有服务降级或者熔断机制。

单独部署，与业务系统尽量解耦，如果直接与业务系统交互，一定要有熔断机制。





使用本地缓存需要注意的问题

1 保证线程安全（@PostContrust单例对象保证线程安全，guava采用currentmap分段锁的形式保证线程安全）

2 要控制缓存大小，不能影响gc性能（guava可以设置最大缓存大小）

3 要有缓存淘汰策略（expireAfterWrite\expireAfterAccess\refreshAfterWrite）

4 不同服务器的缓存同步问题（我们是通过zk来尽量试试通知数据变更、也可以通过mq或者redis的发布订阅实现，同时还有本地刷新策略，还有前端业务控制来达到最终的一致性）

5 guava采用了软指针和弱指针的方式，来避免oom





zk的回调：一次性的、而且通知的仅仅是那个节点、发生了什么类型的变化，但是变化的具体内容需要客户端自己去调用。curator可以实现多次监听。watcher（getdata\exists\getchild等等）是和节点绑定在一起的，也可能是跟客户端session绑定在一起的（在构建客户端session的时候制定watcher即defaultWatcher，可以监听session的状态）



















定时任务升级的方案

1 主节点的单节点问题 2 主节点的性能问题 3 下游系统的处理问题

本地锁和分布式锁

本地锁：sync、reentrantlock、readwritelock、semaphore、countdownlatch、cyclicbarrier

sync和reentrantlock区别

countdownlatch和cyclicbarrier的区别

zk和redis分布式锁的区别

rocketmq事务

线程池



zk选主流程、恢复流程、原子广播流程

zk可以实现什么功能（配置中心-服务的一些properties、选主、分布式锁(都是在持久节点下创建临时顺序目录)、服务管理-dubbo的服务治理）



zk每个客户端在创建的时候会生成一个session，session也会同步到zk集群中，如果发生了网络闪断的情况，客户端会选择一个新的zk服务器地址(在构建客户端时指定)。session也会有超时时间，这个超时时间就是靠心跳维持。



Moved和asked错误

moved-slot的数据已经迁移走了，可以重定向到一个新的服务ip上了，同时客户端会保存下来这个slot和目标ip的关系

asked错误->数据在迁移中，旧的数据还在老的ip中，会直接返回。但是新数据的写入需要定位到新的ip上，老的ip是migrating状态，新的ip是importing状态，只有处于importing状态的才能接收asking命令，然后才能进行后续的写入动作，等slot中的数据都迁移到新的ip之后，客户端访问才返回moved，客户端才会保存这个关系，否则不会保存这个关系。



redis锁-超时、释放不是自己的锁，redsion-watchdog锁续命、主从数据同步问题的redlock















